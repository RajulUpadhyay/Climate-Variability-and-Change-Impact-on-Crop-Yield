{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# New code with adjustments\n"
      ],
      "metadata": {
        "id": "oItDJihPZQHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "from datetime import date, timedelta\n",
        "\n",
        "\n",
        "exception_count = 0\n",
        "\n",
        "starting_time = time.time()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=PendingDeprecationWarning)\n",
        "\n",
        "# Extracting files locations\n",
        "files_H=[]\n",
        "files_F=[]\n",
        "\n",
        "# Observed File\n",
        "sh_observed = pd.read_csv(r'/content/drive/MyDrive/Bias_Correction/Observed_data/IMD_pr_1985_2014.csv',header = None,skiprows=(2))\n",
        "sh_observed.drop(0,axis = 1, inplace = True)\n",
        "sh_observed.columns = [i for i in range(4964)]\n",
        "\n",
        "merge_coordinates = pd.read_csv('/content/drive/MyDrive/Bias_Correction/India_lat_lon_pr.csv')\n",
        "\n",
        "# lat lon of a given observed file\n",
        "f_lat = merge_coordinates['N']\n",
        "f_lon = merge_coordinates['E']\n",
        "\n",
        "# from datetime import date, timedelta\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "\n",
        "\n",
        "# value1 = datetime.date(1976,1,1)\n",
        "dat1   = [date for date in daterange(date(1985,1,1),date(2015,1,1))]\n",
        "\n",
        "files_F_new = [[] for r in range(4)]\n",
        "a = 0\n",
        "for f in range(4):\n",
        "    files_F_new[f].append(files_F[a:a+3])\n",
        "    a+=3\n",
        "\n",
        "fH=0\n",
        "\n",
        "for var in ['pr']:\n",
        "    for scenario in ['ssp126']:\n",
        "        loop_time_start = time.time()\n",
        "\n",
        "\n",
        "        sh_hist = pd.read_csv('/content/drive/MyDrive/Bias_Correction/Raw_data/pr/pr_Historical_1985_2014.csv',header = None,skiprows=(2))\n",
        "\n",
        "        sh_hist.drop(0,axis = 1, inplace = True)\n",
        "        sh_hist.columns = [i for i in range(4964)]\n",
        "        df_H = pd.DataFrame(columns = [r for r in range(4964)])\n",
        "\n",
        "        path = r'/content/drive/MyDrive/Bias_Correction/Raw_data'\n",
        "\n",
        "        file_path = [f'{path}/{var}/{scenario}/{var}_{scenario}_2015_2044.csv',\n",
        "                     f'{path}/{var}/{scenario}/{var}_{scenario}_2045_2074.csv',\n",
        "                     f'{path}/{var}/{scenario}/{var}_{scenario}_2071_2100.csv']\n",
        "\n",
        "        # for fF in range(3):\n",
        "\n",
        "        # if fF == 0 or fF == 3 or fF == 6 or fF == 9:\n",
        "        dat2_0   = [date for date in daterange(date(2015,1,1),date(2045,1,1))]\n",
        "\n",
        "        # elif fF == 1 or fF == 4 or fF == 7 or fF == 10:\n",
        "        dat2_1   = [date for date in daterange(date(2045,1,1),date(2075,1,1))]\n",
        "\n",
        "        # else:\n",
        "        dat2_2   = [date for date in daterange(date(2071,1,1),date(2101,1,1))]\n",
        "\n",
        "        df_F_0 = pd.DataFrame(columns = [r for r in range(4964)])\n",
        "        df_F_1 = pd.DataFrame(columns = [r for r in range(4964)])\n",
        "        df_F_2 = pd.DataFrame(columns = [r for r in range(4964)])\n",
        "\n",
        "        #Reading Excel file\n",
        "        sh_F_0 = pd.read_csv(file_path[0],header = None,skiprows = 2)\n",
        "        sh_F_0.drop(0,axis = 1, inplace = True)\n",
        "        sh_F_0.columns = [i for i in range(4964)]\n",
        "\n",
        "        sh_F_1 = pd.read_csv(file_path[1],header = None,skiprows = 2)\n",
        "        sh_F_1.drop(0,axis = 1, inplace = True)\n",
        "        sh_F_1.columns = [i for i in range(4964)]\n",
        "\n",
        "        sh_F_2 = pd.read_csv(file_path[2],header = None,skiprows = 2)\n",
        "        sh_F_2.drop(0,axis = 1, inplace = True)\n",
        "        sh_F_2.columns = [i for i in range(4964)]\n",
        "\n",
        "        for col in range(4964):\n",
        "            # col = 280\n",
        "            # print('Entered the column loop')\n",
        "            obs = sh_observed[col][:len(dat1)]\n",
        "\n",
        "            sim_hist = sh_hist[col]\n",
        "\n",
        "            sim_future_0 = sh_F_0[col][:len(dat2_0)]\n",
        "            sim_future_1 = sh_F_1[col][:len(dat2_1)]\n",
        "            sim_future_2 = sh_F_2[col][:len(dat2_2)]\n",
        "\n",
        "\n",
        "            o_RainMonth = [[] for i in range(12)]\n",
        "            for m in range(len(o_RainMonth)):\n",
        "                for i in range(len(obs)):\n",
        "                    if dat1[i].month==m+1:\n",
        "                        o_RainMonth[m].append(obs[i])\n",
        "\n",
        "\n",
        "            sh_RainMonth = [[] for i in range(12)]\n",
        "            for m in range(len(sh_RainMonth)):\n",
        "                for i in range(len(sim_hist)):\n",
        "                    if dat1[i].month==m+1:\n",
        "                        sh_RainMonth[m].append(sim_hist[i])\n",
        "\n",
        "\n",
        "            sf_RainMonth_0 = [[] for i in range(12)]\n",
        "            for m in range(len(sf_RainMonth_0)):\n",
        "                for i in range(len(sim_future_0)):\n",
        "                    if dat2_0[i].month==m+1:\n",
        "                        sf_RainMonth_0[m].append(sim_future_0[i])\n",
        "\n",
        "\n",
        "            sf_RainMonth_1 = [[] for i in range(12)]\n",
        "            for m in range(len(sf_RainMonth_1)):\n",
        "                for i in range(len(sim_future_1)):\n",
        "                    if dat2_1[i].month==m+1:\n",
        "                        sf_RainMonth_1[m].append(sim_future_1[i])\n",
        "\n",
        "\n",
        "            sf_RainMonth_2 = [[] for i in range(12)]\n",
        "            for m in range(len(sf_RainMonth_2)):\n",
        "                for i in range(len(sim_future_2)):\n",
        "                    if dat2_2[i].month==m+1:\n",
        "                        sf_RainMonth_2[m].append(sim_future_2[i])\n",
        "\n",
        "\n",
        "            Mon_dat_H = [[] for r in range(12)]\n",
        "            for m in range(len(Mon_dat_H)):\n",
        "                for i in range(len(dat1)):\n",
        "                    if dat1[i].month==m+1:\n",
        "                        Mon_dat_H[m].append(dat1[i])\n",
        "\n",
        "            Mon_dat_F_0 = [[] for r in range(12)]\n",
        "            for m in range(len(Mon_dat_F_0)):\n",
        "                for i in range(len(dat2_0)):\n",
        "                    if dat2_0[i].month==m+1:\n",
        "                        Mon_dat_F_0[m].append(dat2_0[i])\n",
        "\n",
        "            Mon_dat_F_1 = [[] for r in range(12)]\n",
        "            for m in range(len(Mon_dat_F_1)):\n",
        "                for i in range(len(dat2_1)):\n",
        "                    if dat2_1[i].month==m+1:\n",
        "                        Mon_dat_F_1[m].append(dat2_1[i])\n",
        "\n",
        "            Mon_dat_F_2 = [[] for r in range(12)]\n",
        "            for m in range(len(Mon_dat_F_2)):\n",
        "                for i in range(len(dat2_2)):\n",
        "                    if dat2_2[i].month==m+1:\n",
        "                        Mon_dat_F_2[m].append(dat2_2[i])\n",
        "\n",
        "            # Rainfall frequency adjustment\n",
        "\n",
        "            Future_corrected_0 = [[] for data in range(12)]\n",
        "            Future_corrected_1 = [[] for data in range(12)]\n",
        "            Future_corrected_2 = [[] for data in range(12)]\n",
        "            Final_corrected_H = [[] for data in range(12)]\n",
        "            Final_corrected_F_0 = [[] for data in range(12)]\n",
        "            Final_corrected_F_1 = [[] for data in range(12)]\n",
        "            Final_corrected_F_2 = [[] for data in range(12)]\n",
        "\n",
        "            for month in range(12):\n",
        "                o_flag = False\n",
        "                sh_flag = False\n",
        "                sf_0_flag = False\n",
        "                sf_1_flag = False\n",
        "                sf_2_flag = False\n",
        "\n",
        "                x = o_RainMonth[month]\n",
        "                x_TH = [s for s in x if s>0]\n",
        "\n",
        "                test_set = set(x_TH)\n",
        "                if len(x_TH) == 0 or len(x_TH) == 1 or len(test_set) == 1:\n",
        "                    o_flag = True\n",
        "\n",
        "                y = sh_RainMonth[month]\n",
        "                y_sorted = sorted(y)\n",
        "\n",
        "                # print ('Observed_mean_without_zeroes:{}'.format(np.mean(x_TH)))\n",
        "\n",
        "                y_TH = [s for s in y if s>0]\n",
        "\n",
        "                # Rainfall frequency adjustment\n",
        "\n",
        "                i = 0\n",
        "                while len(y_TH)>len(x_TH):\n",
        "                    data = y_sorted[i]\n",
        "                    y_TH = []\n",
        "                    y_TH = [s for s in y if s>data]\n",
        "                    if len(y_TH) == len(x_TH):\n",
        "                        Threshold = data\n",
        "                        break\n",
        "                    Threshold = y_sorted[i]\n",
        "                    i+=1\n",
        "\n",
        "\n",
        "                test_set = set(y_TH)\n",
        "                if len(y_TH)== 0 or len(y_TH) == 1 or len(test_set) == 1: sh_flag = True\n",
        "\n",
        "                if o_flag == False and sh_flag == False:\n",
        "\n",
        "                    z_0  = sf_RainMonth_0[month]\n",
        "                    z_TH_0 = [s for s in z_0 if s>Threshold]\n",
        "                    test_set = set(z_TH_0)\n",
        "                    if len(z_TH_0)== 0 or len(z_TH_0) == 1 or len(test_set) == 1: sf_0_flag = True\n",
        "\n",
        "                    z_1  = sf_RainMonth_1[month]\n",
        "                    z_TH_1 = [s for s in z_1 if s>Threshold]\n",
        "                    test_set = set(z_TH_1)\n",
        "                    if len(z_TH_1)== 0 or len(z_TH_0) == 1 or len(test_set) == 1: sf_1_flag = True\n",
        "\n",
        "                    z_2  = sf_RainMonth_2[month]\n",
        "                    z_TH_2 = [s for s in z_2 if s>Threshold]\n",
        "                    test_set = set(z_TH_2)\n",
        "                    if len(z_TH_2)== 0 or len(z_TH_0) == 1 or len(test_set) == 1: sf_2_flag = True\n",
        "\n",
        "                    # Fitting gamma distribution and CDF calculations\n",
        "\n",
        "                    fit_alpha_x, fit_loc_x, fit_beta_x = st.gamma.fit(x_TH,floc = 0)\n",
        "                    Observed = st.gamma.cdf(x_TH, a = fit_alpha_x, loc = fit_loc_x, scale = fit_beta_x)\n",
        "\n",
        "\n",
        "                    fit_alpha_y, fit_loc_y, fit_beta_y = st.gamma.fit(y_TH,floc = 0)\n",
        "                    Simulated_Historical = st.gamma.cdf(y_TH, a = fit_alpha_y, loc = fit_loc_y, scale = fit_beta_y)\n",
        "\n",
        "                    if sf_0_flag == False:\n",
        "                        fit_alpha_z_0, fit_loc_z_0, fit_beta_z_0 = st.gamma.fit(z_TH_0,floc = 0)\n",
        "                        F_to_corrected_0 = st.gamma.cdf(z_TH_0, a = fit_alpha_z_0, loc = fit_loc_z_0, scale = fit_beta_z_0)\n",
        "\n",
        "                    if sf_1_flag == False:\n",
        "                        fit_alpha_z_1, fit_loc_z_1, fit_beta_z_1 = st.gamma.fit(z_TH_1,floc = 0)\n",
        "                        F_to_corrected_1 = st.gamma.cdf(z_TH_1, a = fit_alpha_z_1, loc = fit_loc_z_1, scale = fit_beta_z_1)\n",
        "\n",
        "                    if sf_2_flag == False:\n",
        "                        fit_alpha_z_2, fit_loc_z_2, fit_beta_z_2 = st.gamma.fit(z_TH_2,floc = 0)\n",
        "                        F_to_corrected_2 = st.gamma.cdf(z_TH_2, a = fit_alpha_z_2, loc = fit_loc_z_2, scale = fit_beta_z_2)\n",
        "\n",
        "                    # Quantile or inverse CDF calculation\n",
        "\n",
        "                    Past_Corrected  = st.gamma.ppf(Simulated_Historical, a = fit_alpha_x, loc = fit_loc_x, scale = fit_beta_x)\n",
        "                    Past_Corrected = np.round(Past_Corrected,2)\n",
        "\n",
        "                    if sf_0_flag == False:\n",
        "                        F_corrected_with_x_0 = st.gamma.ppf(F_to_corrected_0, a = fit_alpha_x, loc = fit_loc_x, scale = fit_beta_x)\n",
        "                        F_corrected_with_y_0 = st.gamma.ppf(F_to_corrected_0, a = fit_alpha_y, loc = fit_loc_y, scale = fit_beta_y)\n",
        "\n",
        "                    if sf_1_flag == False:\n",
        "                        F_corrected_with_x_1 = st.gamma.ppf(F_to_corrected_1, a = fit_alpha_x, loc = fit_loc_x, scale = fit_beta_x)\n",
        "                        F_corrected_with_y_1 = st.gamma.ppf(F_to_corrected_1, a = fit_alpha_y, loc = fit_loc_y, scale = fit_beta_y)\n",
        "\n",
        "                    if sf_2_flag == False:\n",
        "                        F_corrected_with_x_2 = st.gamma.ppf(F_to_corrected_2, a = fit_alpha_x, loc = fit_loc_x, scale = fit_beta_x)\n",
        "                        F_corrected_with_y_2 = st.gamma.ppf(F_to_corrected_2, a = fit_alpha_y, loc = fit_loc_y, scale = fit_beta_y)\n",
        "\n",
        "                    warnings.filterwarnings('error')\n",
        "\n",
        "                    if sf_0_flag == False:\n",
        "                        d_0 = []\n",
        "                        for dd in range(len(F_corrected_with_y_0)):\n",
        "\n",
        "                            if ( np.array(z_TH_0)[dd] - F_corrected_with_y_0[dd]) >=0:\n",
        "                                d_0.append(np.array(z_TH_0)[dd] - F_corrected_with_y_0[dd])\n",
        "\n",
        "                            else:\n",
        "                                d_0.append(0)\n",
        "\n",
        "\n",
        "                    if sf_1_flag == False:\n",
        "                        d_1 = []\n",
        "                        for dd in range(len(F_corrected_with_y_1)):\n",
        "                            if ( np.array(z_TH_1)[dd] - F_corrected_with_y_1[dd]) >=0:\n",
        "                                d_1.append(np.array(z_TH_1)[dd] - F_corrected_with_y_1[dd])\n",
        "                            else:\n",
        "                                d_1.append(0)\n",
        "\n",
        "\n",
        "                    if sf_2_flag == False:\n",
        "                        d_2 = []\n",
        "                        for dd in range(len(F_corrected_with_y_2)):\n",
        "                            if ( np.array(z_TH_2)[dd] - F_corrected_with_y_2[dd]) >=0:\n",
        "                                d_2.append(np.array(z_TH_2)[dd] - F_corrected_with_y_2[dd])\n",
        "                            else:\n",
        "                                d_2.append(0)\n",
        "\n",
        "\n",
        "\n",
        "                    if sf_0_flag == False:\n",
        "                        for data1 in range(len(F_corrected_with_x_0)):\n",
        "                            Future_corrected_0[month].append(F_corrected_with_x_0[data1] + np.array(d_0[data1]))\n",
        "\n",
        "                    if sf_1_flag == False:\n",
        "                        for data1 in range(len(F_corrected_with_x_1)):\n",
        "                            Future_corrected_1[month].append(F_corrected_with_x_1[data1] + np.array(d_1[data1]))\n",
        "\n",
        "                    if sf_2_flag == False:\n",
        "                        for data1 in range(len(F_corrected_with_x_2)):\n",
        "                            Future_corrected_2[month].append(F_corrected_with_x_2[data1] + np.array(d_2[data1]))\n",
        "\n",
        "\n",
        "                    ### Put back corrected data in timeseries\n",
        "\n",
        "                    y_dates_H = []\n",
        "                    for day in range(len(y)):\n",
        "                        if y[day]>Threshold:\n",
        "                            y_dates_H.append(Mon_dat_H[month][day])\n",
        "\n",
        "                    if sf_0_flag == False:\n",
        "                        z_dates_F_0 = []\n",
        "                        for day in range(len(z_0)):\n",
        "                            #if z[day]>Threshold[col][month][0]:\n",
        "                            if z_0[day]>Threshold:\n",
        "                                z_dates_F_0.append(Mon_dat_F_0[month][day])\n",
        "\n",
        "                    if sf_1_flag == False:\n",
        "                        z_dates_F_1 = []\n",
        "                        for day in range(len(z_1)):\n",
        "                            #if z[day]>Threshold[col][month][0]:\n",
        "                            if z_1[day]>Threshold:\n",
        "                                z_dates_F_1.append(Mon_dat_F_1[month][day])\n",
        "\n",
        "                    if sf_2_flag == False:\n",
        "                        z_dates_F_2 = []\n",
        "                        for day in range(len(z_2)):\n",
        "                            #if z[day]>Threshold[col][month][0]:\n",
        "                            if z_2[day]>Threshold:\n",
        "                                z_dates_F_2.append(Mon_dat_F_2[month][day])\n",
        "\n",
        "                    key1 = 0\n",
        "                    for dt in range(len(Mon_dat_H[month])):\n",
        "                        if Mon_dat_H[month][dt] in y_dates_H:\n",
        "                            Final_corrected_H[month].append(Past_Corrected[key1])\n",
        "                            key1 +=1\n",
        "                        else:\n",
        "                            Final_corrected_H[month].append(0)\n",
        "\n",
        "                    if sf_0_flag == False:\n",
        "                        key2_0 = 0\n",
        "                        for dt in range(len(Mon_dat_F_0[month])):\n",
        "                            if Mon_dat_F_0[month][dt] in z_dates_F_0:\n",
        "                                Final_corrected_F_0[month].append(Future_corrected_0[month][key2_0])\n",
        "                                key2_0 +=1\n",
        "                            else:\n",
        "                                Final_corrected_F_0[month].append(0)\n",
        "                    else:\n",
        "                      exception_count+=1\n",
        "                      Final_corrected_F_0[month] = sf_RainMonth_0[month]\n",
        "\n",
        "\n",
        "                    if sf_1_flag == False:\n",
        "                        key2_1 = 0\n",
        "                        for dt in range(len(Mon_dat_F_1[month])):\n",
        "                            if Mon_dat_F_1[month][dt] in z_dates_F_1:\n",
        "                                Final_corrected_F_1[month].append(Future_corrected_1[month][key2_1])\n",
        "                                key2_1 +=1\n",
        "                            else:\n",
        "                                Final_corrected_F_1[month].append(0)\n",
        "                    else:\n",
        "                      exception_count+=1\n",
        "                      Final_corrected_F_1[month] = sf_RainMonth_1[month]\n",
        "\n",
        "                    if sf_2_flag == False:\n",
        "                        key2_2 = 0\n",
        "                        for dt in range(len(Mon_dat_F_2[month])):\n",
        "                            if Mon_dat_F_2[month][dt] in z_dates_F_2:\n",
        "                                Final_corrected_F_2[month].append(Future_corrected_2[month][key2_2])\n",
        "                                key2_2 +=1\n",
        "                            else:\n",
        "                                Final_corrected_F_2[month].append(0)\n",
        "                    else:\n",
        "                      exception_count+=1\n",
        "                      Final_corrected_F_2[month] = sf_RainMonth_2[month]\n",
        "\n",
        "                else:\n",
        "\n",
        "                    exception_count+=1\n",
        "                    for dt in range(len(Mon_dat_H[month])):\n",
        "                        Final_corrected_H[month].append(0)\n",
        "\n",
        "                    # for dt in range(len(Mon_dat_F_0[month])):\n",
        "                    Final_corrected_F_0[month] = sf_RainMonth_0[month]\n",
        "\n",
        "                    # for dt in range(len(Mon_dat_F_1[month])):\n",
        "                    Final_corrected_F_1[month] = sf_RainMonth_1[month]\n",
        "\n",
        "                    # for dt in range(len(Mon_dat_F_2[month])):\n",
        "                    Final_corrected_F_2[month] = sf_RainMonth_2[month]\n",
        "\n",
        "            Grid_timeseries_H = []\n",
        "            years_list_H = list(range(min(dat1).year, max(dat1).year+1))\n",
        "            for yearH in years_list_H:\n",
        "                for monthH in range(len(Final_corrected_H)):\n",
        "                    for dayH in range(len(Final_corrected_H[monthH])):\n",
        "                        if Mon_dat_H[monthH][dayH].year == yearH:\n",
        "                                if Mon_dat_H[monthH][dayH].month == monthH+1:\n",
        "                                    Grid_timeseries_H.append(Final_corrected_H[monthH][dayH])\n",
        "\n",
        "\n",
        "            Grid_timeseries_F_0 = []\n",
        "            years_list_F_0 = list(range(min(dat2_0).year, max(dat2_0).year+1))\n",
        "            for yearF in years_list_F_0:\n",
        "                for monthF in range(len(Final_corrected_F_0)):\n",
        "                    for dayF in range(len(Final_corrected_F_0[monthF])):\n",
        "                        if Mon_dat_F_0[monthF][dayF].year == yearF:\n",
        "                                if Mon_dat_F_0[monthF][dayF].month == monthF+1:\n",
        "                                    Grid_timeseries_F_0.append(Final_corrected_F_0[monthF][dayF])\n",
        "\n",
        "\n",
        "            Grid_timeseries_F_1 = []\n",
        "            years_list_F_1 = list(range(min(dat2_1).year, max(dat2_1).year+1))\n",
        "            for yearF in years_list_F_1:\n",
        "                for monthF in range(len(Final_corrected_F_1)):\n",
        "                    for dayF in range(len(Final_corrected_F_1[monthF])):\n",
        "                        if Mon_dat_F_1[monthF][dayF].year == yearF:\n",
        "                                if Mon_dat_F_1[monthF][dayF].month == monthF+1:\n",
        "                                    Grid_timeseries_F_1.append(Final_corrected_F_1[monthF][dayF])\n",
        "\n",
        "            Grid_timeseries_F_2 = []\n",
        "            years_list_F_2 = list(range(min(dat2_2).year, max(dat2_2).year+1))\n",
        "            for yearF in years_list_F_2:\n",
        "                for monthF in range(len(Final_corrected_F_2)):\n",
        "                    for dayF in range(len(Final_corrected_F_2[monthF])):\n",
        "                        if Mon_dat_F_2[monthF][dayF].year == yearF:\n",
        "                                if Mon_dat_F_2[monthF][dayF].month == monthF+1:\n",
        "                                    Grid_timeseries_F_2.append(Final_corrected_F_2[monthF][dayF])\n",
        "\n",
        "            df_H[col] = np.around(Grid_timeseries_H, decimals = 2)\n",
        "            df_F_0[col] = np.around(Grid_timeseries_F_0, decimals = 2)\n",
        "            df_F_1[col] = np.around(Grid_timeseries_F_1, decimals = 2)\n",
        "            df_F_2[col] = np.around(Grid_timeseries_F_2, decimals = 2)\n"
      ],
      "metadata": {
        "id": "P3mPQzlvkZiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(exception_count)"
      ],
      "metadata": {
        "id": "IUB0v5gwloKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418d65ec-e419-4b91-ffd4-5d0906bdafe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = r'/content/drive/MyDrive/'\n",
        "\n",
        "df_F_0.loc[-2] = f_lat\n",
        "df_F_0.loc[-1] = f_lon\n",
        "df_F_0.sort_index(inplace =True)\n",
        "df_F_0.to_csv(f'{save_path}/{var}_{scenario}_2015_2044_corrected_2.csv',index=False,header = None)\n",
        "\n",
        "df_F_1.loc[-2] = f_lat\n",
        "df_F_1.loc[-1] = f_lon\n",
        "df_F_1.sort_index(inplace =True)\n",
        "df_F_1.to_csv(f'{save_path}/{var}_{scenario}_2045_2074_corrected_2.csv',index=False,header = None)\n",
        "\n",
        "\n",
        "df_F_2.loc[-2] = f_lat\n",
        "df_F_2.loc[-1] = f_lon\n",
        "df_F_2.sort_index(inplace =True)\n",
        "df_F_2.to_csv(f'{save_path}/{var}_{scenario}_2071_2100_corrected_2.csv',index=False,header = None)\n",
        "# print('wrote file for corrected future file for iteration')\n",
        "loop_time_end = time.time()\n",
        "print(f'Time taken in minutes for {scenario} scenario:',(loop_time_end - loop_time_start)/60)\n",
        "\n",
        "df_H.loc[-2] = f_lat\n",
        "df_H.loc[-1] = f_lon\n",
        "df_H.sort_index(inplace =True)\n",
        "df_H.to_csv(f'{save_path}/{var}_{scenario}_1985_2014_corrected_2.csv',index=False,header = None)\n",
        "print('wrote file for corrected historical file')\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print('Finished writing all the files')\n",
        "# print('Time taken in minutes:',(end_time - starting_time)/60)\n",
        "print('Time taken in hours:',(end_time - starting_time)/3600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AfpG2tqZPtD",
        "outputId": "6ce83e32-7b91-4d92-8408-efc38fb56365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken in minutes for ssp126 scenario: 326.6917207757632\n",
            "wrote file for corrected historical file\n",
            "Finished writing all the files\n",
            "Time taken in hours: 5.500790570709441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(col)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9qZl6BTZPZB",
        "outputId": "073a446a-dfef-44d7-f3c7-8763e77addb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(col)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOBPcHRn-SM2",
        "outputId": "9e728ee8-8b27-479c-9628-3b6f910b097c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_loc_z_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3bYW6amDlDV",
        "outputId": "36e7c40d-3566-4f3a-ee7a-2b78b8c22adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_beta_z_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea9FMTEKDnbi",
        "outputId": "1f0e612b-45be-4016-aef6-c0c1a5bc53cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.934822014693184"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}